{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0a456efd1d2e2bfa10bfdad488db5626e5f8bd233a0f11ae70ce0e7717a6a7d8d",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Natural Language Processing\n",
    "    - How to deal with text data.\n",
    "    - It falls on Ai (computer performing task that a human can do.)\n",
    "    - Includes:\n",
    "        - sentimental analysis.\n",
    "        - Topic Modeling\n",
    "        - Text Generation\n",
    "        - etc."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* Natural Language -> English,nepali language\n",
    "* Processing -> How a computer carries out instructions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Data Sciene\n",
    "    - Using data to make decision.\n",
    "* Programming: \n",
    "    - pandas,sklearn,re,nltk,TextBlob, gensim\n",
    "* Math & Stats:\n",
    "    - corpus,DTM,word counts, sentimental analysis, topic modeling, text generation\n",
    "* Communication:\n",
    "    - scope,visualize,extract insights expertise."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Data Science Workflow : Steps\n",
    "    - Start with a Question\n",
    "    - Get and clean the data\n",
    "    - perfomr EDA\n",
    "    - Apply Techniques\n",
    "    - Share Insights"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 2. Data Cleaning\n",
    "* Goal:\n",
    "    - Get the data in a clean, standard format for further analaysis.\n",
    "    - Different types of analaysis require Different data format.\n",
    "        - Corpus -> Collection of text.\n",
    "        - Docuemnt-Term Matrix:\n",
    "            - 1. Clean Text: remove excess, unnecessary parts of the text, remove number, lower case, remove punctuation.\n",
    "            - 2. Tokenize Text : split the text into similar pieces\n",
    "                - Stop word: all, a, the, here, bye tec.\n",
    "            - 3. DTM : put into matrix so a machine can read it.\n",
    "            - 4. Liminatiztion & stemming: merge similar words(drive,drives, driving)\n",
    "    - ![](files/1.png)\n",
    "    - Python Library:\n",
    "        - scikit learn: for machine learning\n",
    "        - count-vectorizer: sklearn way to make a DTM.\n",
    "    - ![](files/2.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 3. Exploratory Data Analysis.\n",
    "    - Input: A corpus and a DTM\n",
    "    - EDA: Summarize the main characteristics of the data set, often using visual methods.\n",
    "    - Output: Figure out the main trends in the data if it makes sense.\n",
    "### What are some ways you think of explore of data.\n",
    "    - Top words: Find the most common words:\n",
    "    - Vocabulary: Take a look at the unique no.of words used.\n",
    "    - Amount of profonity: Note the no.of swear words said.\n",
    "    - etc...\n",
    "\n",
    "## EDA Steps:\n",
    "    - 1. Data: Determine the format of the raw data.\n",
    "    - 2. Aggregate: Figure out how to Aggregate the data.\n",
    "    - 3. Visualize : Find the best way to visualize . [wordcloud,matplotlib]\n",
    "    - 4. Insights:\n",
    "        - Extract some key takeway from visualization.\n",
    "        - Does the data make sense.\n",
    "        - Do we need to clean the data again."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 4. NLP Techniques\n",
    "    - 1. Input: A corpus. The reason we're not using the document-term matrix. Here is because order matter. \n",
    "        - \"great\" = positive\n",
    "        - \" not great\" = negative\n",
    "    - 2. TextBlob : It is a python library that is built on top of nltk. It's easier to use and provides some additional funcionality, such as rules-based sentiment.\n",
    "    - 3. Output: For each comedian, we will give them a sentiment score(how positive/negative are they) and a subjectivity score ( how opinionated are they.)\n",
    "        - eg: TextBlob(' i loove python').sentiment\n",
    "                - Sentiment(polarity=0.5, subjectivity=0.6)\n",
    "- ![](files/3.png)\n",
    "- ![](files/4.png)\n",
    "- TextBlob finds all the words and phrases that it can assign a polarity and subjectivity to, and averages all of them together.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 4. Topic Modeling ( no. of iteration, no. of topics) -> gensim\n",
    "    - Input: A DTM. Each topic will consist of a set of words, where order does't matter, so we're going to start with the bag of words format.\n",
    "    - Gensim: It is a python toolkit for topic modeling.\n",
    "        - We are going to a popular topic modeling technique called LDA.\n",
    "        - Also nltk for some part of speech tagging.\n",
    "    - Output: To find themes accross various comedy routines, and see which person/title tend to talk about which theme.\n",
    "- ![](files/5.png)\n",
    "- ![](files/6.png)\n",
    "## How LDA works.\n",
    "    - Goal: We want LDA to learn the topic mix in each document, and the word mix in each topic.\n",
    "    - Choose the number of topics you think there are in your corpus.\n",
    "        - K :2\n",
    "    - Randomly assign each words in each document to one of 2 topics.\n",
    "        - The word 'banana' in Document #1 is Randomly assigned to Topic B\n",
    "    - Go through every word and its topic assignment in each document. look how often the topic occurs in the doucment and how often the word occurs in the topic overall. Based on this inof, assign the word a new topic.\n",
    "* input: DTM, number of topics, number of iterations.\n",
    "* Gensim will go through the process of finding the best word distribution for each topic and best topic distribution for each document.\n",
    "* Output: The topic words in each topic. It is your job as human to interpret this and see if the results make sense. If not try altering the paramenters - terms in the DTM, no. of topics. no. of iterations etc. Stop when the topic make sense.\n",
    "* This is a probabilistic approach to topic modeling. There are also matrix factorization techniques for topic modeling suchas LSI and NMF."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Text Generation\n",
    "* Markov Chains\n",
    "    - It is a way of representing how system change over time.\n",
    "    - The main concept behind Markov chains are that they are memoryless, meaning that the next state of a process only depends on the previous state.\n",
    "    - Create a dictionary for a corpus where the keys are teh current state and the values are the options for the next state. Write a function to randomly generate next terms.\n",
    "    - Output: Comedy routines in the style in the style of each actor.\n",
    "    - This is an oversimplified way of generating text. A much more complex technique would be to use deep learning and LSTM. This not only takes into account the previous sord, but things like the words before the previous word, what words have already been generated. etc."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* ![](files/7.png)\n",
    "* ![](files/8.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}