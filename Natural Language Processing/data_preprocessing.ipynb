{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "1. Data: Bunch of raw information on which operations will be performed.(pdf,txt,csv)\r\n",
    "2. Data Pre-processing: way to develop informative data from raw data by removing unwanted attributes.\r\n",
    "3. Types:\r\n",
    "    - Remove or fill null values.\r\n",
    "    - Count unique values in column\r\n",
    "    - Drop irrelevant columns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenization\r\n",
    "    - Tokeniztion method is used to split a phrase, sentence, paragraph or entire text document into smaller units.(words or terms.)\r\n",
    "    - Each of these smaller units are called tokens.\r\n",
    "* eg:\r\n",
    "    - text = 'This is a cake'\r\n",
    "    - after tokenization:\r\n",
    "        - [ 'this' , 'is' , 'a' , 'cake' ]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import nltk\r\n",
    "text = 'this is a cake. I love it'\r\n",
    "# line tokenization\r\n",
    "tokens = nltk.sent_tokenize(text)\r\n",
    "print('line tokenization:',tokens)\r\n",
    "\r\n",
    "# wrod tokenization\r\n",
    "tokens = nltk.word_tokenize(text)\r\n",
    "print('word tokeniztion:',tokens) # list\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "line tokenization: ['this is a cake.', 'I love it']\n",
      "word tokeniztion: ['this', 'is', 'a', 'cake', '.', 'I', 'love', 'it']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stemming\r\n",
    "    - Stemming is the way to reduce a word to its word stem that affixes to suffixes and prefixes.\r\n",
    "    - It works by cutting off the end or the beginning of the word while taking info account a list of common prefixes and suffixes that can be found in an inflected word.\r\n",
    "\r\n",
    "|Form|Suffix|stem|\r\n",
    "|---|---|---|\r\n",
    "|Cats|-s|cat|\r\n",
    "|Birds|-s|bird|\r\n",
    "\r\n",
    "* Why do we need stemming?\r\n",
    "    - less input data\r\n",
    "    - ml works better with it\r\n",
    "    - makes training data more dense\r\n",
    "    - reduce size of dictionary\r\n",
    "    - helps to normalize the words in the document."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from nltk.stem import PorterStemmer\r\n",
    "\r\n",
    "ps = PorterStemmer()\r\n",
    "words = ['like','likes','liking','liked']\r\n",
    "stem_words = [ps.stem(word) for word in words]\r\n",
    "stem_words"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['like', 'like', 'like', 'like']"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lemmatization\r\n",
    "    - It helps to do morphological analysis of the words.\r\n",
    "    - It is important to have the knowledge about the detailed dictionaries which the algorithm can refer to link the form back to its lemma.\r\n",
    "|Form|Morphological info|Lemma|\r\n",
    "|----|----|----|\r\n",
    "|Helps|Third person singular number,present tense help|Help|\r\n",
    "|Helping|ing form of the verb|Help|"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Difference between stemming and lemmatization\r\n",
    "|Topic|Stemming|Lemmatization|\r\n",
    "|---|---|---|\r\n",
    "|Goal|Reduce infelectional forms (Stemming refers to the crude heruristic process which chops off the ends of the words in order to achieve the goal correctly)|Redice infelectional forms(Lemmatiztion refers to do the things properly with the help if a vocabulary and morphological analysis of words)|\r\n",
    "|Implementation|Stemmers are typically easier to impelement and run faster compare to lemmatization|Lemmatization is difficult to implement|"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "import nltk\r\n",
    "nltk.download('wordnet')\r\n",
    "\r\n",
    "from nltk.stem import WordNetLemmatizer\r\n",
    "lemmati = WordNetLemmatizer()\r\n",
    "\r\n",
    "print('rides:',lemmati.lemmatize('rides'))\r\n",
    "print('sleeping:',lemmati.lemmatize('sleeping'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "rides: ride\n",
      "sleeping: sleeping\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rabin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stop words\r\n",
    "    - common words that occur in sentences that add weight to the sentence are known as stopwords.\r\n",
    "    - These stop words act as a bride and ensure that sentences are garmmatically correct.\r\n",
    "    - Words that are filter out before precessing natural language."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "from nltk.corpus import stopwords\r\n",
    "import nltk\r\n",
    "nltk.download('stopwords')\r\n",
    "nltk.download('punkt')\r\n",
    "\r\n",
    "stop_words = set(stopwords.words('english'))\r\n",
    "data = 'data science is one of the most trendind field to work with. It needs data to give prediction by using the past scenarios'\r\n",
    "data = nltk.word_tokenize(data)\r\n",
    "\r\n",
    "print('removed word:',[word for word in data if word in stop_words])\r\n",
    "# after removed stop words\r\n",
    "print('removed word:',[word for word in data if word not in stop_words])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "removed word: ['is', 'of', 'the', 'most', 'to', 'with', 'to', 'by', 'the']\n",
      "removed word: ['data', 'science', 'one', 'trendind', 'field', 'work', '.', 'It', 'needs', 'data', 'give', 'prediction', 'using', 'past', 'scenarios']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rabin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rabin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)"
  },
  "interpreter": {
   "hash": "a456efd1d2e2bfa10bfdad488db5626e5f8bd233a0f11ae70ce0e7717a6a7d8d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}