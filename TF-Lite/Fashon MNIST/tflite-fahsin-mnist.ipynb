{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sys import getsizeof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_file_size(file_path):\n",
    "    size = os.path.getsize(file_path)\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bytes(size, unit=None):\n",
    "    if unit == \"KB\":\n",
    "        return print('File size: ' + str(round(size / 1024, 3)) + ' Kilobytes')\n",
    "    elif unit == \"MB\":\n",
    "        return print('File size: ' + str(round(size / (1024 * 1024), 3)) + ' Megabytes')\n",
    "    else:\n",
    "        return print('File size: ' + str(size) + ' bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the Fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Ankle boot'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.imshow(train_images[88], cmap=\"gray\")\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "class_names[train_labels[88]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Build & Compile the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss= SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 6s 2ms/step - loss: 0.4965 - accuracy: 0.8254\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3695 - accuracy: 0.8679\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3364 - accuracy: 0.8784\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3120 - accuracy: 0.8851\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2929 - accuracy: 0.8919\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2793 - accuracy: 0.8960\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2673 - accuracy: 0.9011\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2566 - accuracy: 0.9042\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2479 - accuracy: 0.9062\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2384 - accuracy: 0.9107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17f2c5e3760>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "KERAS_MODEL_NAME = \"tf_model_fashion_mnist.h5\"\n",
    "model.save(KERAS_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 1.19 Megabytes\n"
     ]
    }
   ],
   "source": [
    "convert_bytes(get_file_size(KERAS_MODEL_NAME), \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keras_model_size = get_file_size(KERAS_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.3495 - accuracy: 0.8788\n",
      "\n",
      "Test accuracy is 87.88%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "print('\\nTest accuracy is {}%'.format(round(100*test_acc, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Lite Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\rabin\\AppData\\Local\\Temp\\tmpl_drlgs4\\assets\n"
     ]
    }
   ],
   "source": [
    "TF_LITE_MODEL_FILE_NAME = \"tf_lite_model.tflite\"\n",
    "\n",
    "tf_lite_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = tf_lite_converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "408544"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_model_name = TF_LITE_MODEL_FILE_NAME\n",
    "open(tflite_model_name, \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 398.969 Kilobytes\n"
     ]
    }
   ],
   "source": [
    "convert_bytes(get_file_size(TF_LITE_MODEL_FILE_NAME), \"KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "408544"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_file_size = get_file_size(TF_LITE_MODEL_FILE_NAME)\n",
    "tflite_file_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Input Tensor Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: [ 1 28 28]\n",
      "Input Type: <class 'numpy.float32'>\n",
      "Output Shape: [ 1 10]\n",
      "Output Type: <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path = TF_LITE_MODEL_FILE_NAME)\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "print(\"Input Shape:\", input_details[0]['shape'])\n",
    "print(\"Input Type:\", input_details[0]['dtype'])\n",
    "print(\"Output Shape:\", output_details[0]['shape'])\n",
    "print(\"Output Type:\", output_details[0]['dtype'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resize Tensor Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: [10000    28    28]\n",
      "Input Type: <class 'numpy.float32'>\n",
      "Output Shape: [10000    10]\n",
      "Output Type: <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "interpreter.resize_tensor_input(input_details[0]['index'], (10000, 28, 28))\n",
    "interpreter.resize_tensor_input(output_details[0]['index'], (10000, 10))\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "print(\"Input Shape:\", input_details[0]['shape'])\n",
    "print(\"Input Type:\", input_details[0]['dtype'])\n",
    "print(\"Output Shape:\", output_details[0]['shape'])\n",
    "print(\"Output Type:\", output_details[0]['dtype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs_numpy = np.array(test_images, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_imgs_numpy.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction results shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "interpreter.set_tensor(input_details[0]['index'], test_imgs_numpy)\n",
    "interpreter.invoke()\n",
    "tflite_model_predictions = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(\"Prediction results shape:\", tflite_model_predictions.shape)\n",
    "prediction_classes = np.argmax(tflite_model_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy TFLITE model is 87.88%\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(prediction_classes, test_labels)\n",
    "\n",
    "print('Test accuracy TFLITE model is {}%'.format(round(100*acc, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Lite Model Float 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_LITE_MODEL_FLOAT_16_FILE_NAME = \"tf_lite_float_16_model.tflite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\rabin\\AppData\\Local\\Temp\\tmpo2zah77f\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\rabin\\AppData\\Local\\Temp\\tmpo2zah77f\\assets\n"
     ]
    }
   ],
   "source": [
    "tf_lite_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tf_lite_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tf_lite_converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_model = tf_lite_converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205696"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_model_name = TF_LITE_MODEL_FLOAT_16_FILE_NAME\n",
    "open(tflite_model_name, \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 200.875 Kilobytes\n"
     ]
    }
   ],
   "source": [
    "convert_bytes(get_file_size(TF_LITE_MODEL_FLOAT_16_FILE_NAME), \"KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1648627853295717"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_float_16_file_size = get_file_size(TF_LITE_MODEL_FLOAT_16_FILE_NAME)\n",
    "tflite_float_16_file_size/keras_model_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5034855486801911"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_float_16_file_size/tflite_file_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Lite Size Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_LITE_SIZE_QUANT_MODEL_FILE_NAME = \"tf_lite_quant_model.tflite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\rabin\\AppData\\Local\\Temp\\tmpp4i0kcu1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\rabin\\AppData\\Local\\Temp\\tmpp4i0kcu1\\assets\n",
      "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n",
      "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n"
     ]
    }
   ],
   "source": [
    "tf_lite_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tf_lite_converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "tflite_model = tf_lite_converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103840"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_model_name = TF_LITE_SIZE_QUANT_MODEL_FILE_NAME\n",
    "open(tflite_model_name, \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 101.406 Kilobytes\n"
     ]
    }
   ],
   "source": [
    "convert_bytes(get_file_size(TF_LITE_SIZE_QUANT_MODEL_FILE_NAME), \"KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08322646832521159"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_float_quant_file_size = get_file_size(TF_LITE_SIZE_QUANT_MODEL_FILE_NAME)\n",
    "\n",
    "tflite_float_quant_file_size/keras_model_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5048226509023024"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_float_quant_file_size/ tflite_float_16_file_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy of the Quantized Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Input Tensor Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: [ 1 28 28]\n",
      "Input Type: <class 'numpy.float32'>\n",
      "Output Shape: [ 1 10]\n",
      "Output Type: <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path = TF_LITE_SIZE_QUANT_MODEL_FILE_NAME)\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "print(\"Input Shape:\", input_details[0]['shape'])\n",
    "print(\"Input Type:\", input_details[0]['dtype'])\n",
    "print(\"Output Shape:\", output_details[0]['shape'])\n",
    "print(\"Output Type:\", output_details[0]['dtype'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resize Tensor Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: [10000    28    28]\n",
      "Input Type: <class 'numpy.float32'>\n",
      "Output Shape: [10000    10]\n",
      "Output Type: <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "interpreter.resize_tensor_input(input_details[0]['index'], (10000, 28, 28))\n",
    "interpreter.resize_tensor_input(output_details[0]['index'], (10000, 10))\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "print(\"Input Shape:\", input_details[0]['shape'])\n",
    "print(\"Input Type:\", input_details[0]['dtype'])\n",
    "print(\"Output Shape:\", output_details[0]['shape'])\n",
    "print(\"Output Type:\", output_details[0]['dtype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs_numpy = np.array(test_images, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction results shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "interpreter.set_tensor(input_details[0]['index'], test_imgs_numpy)\n",
    "interpreter.invoke()\n",
    "tflite_model_predictions = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(\"Prediction results shape:\", tflite_model_predictions.shape)\n",
    "prediction_classes = np.argmax(tflite_model_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy TFLITE Quantized model is 87.88%\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(prediction_classes, test_labels)\n",
    "print('Test accuracy TFLITE Quantized model is {}%'.format(round(100*acc, 2)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a456efd1d2e2bfa10bfdad488db5626e5f8bd233a0f11ae70ce0e7717a6a7d8d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
